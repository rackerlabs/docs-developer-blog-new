---
layout: post
title: Your JVM – The Importance of Sizing
date: '2015-11-18 10:00'
comments: true
author: Jonathan Hurley
published: true
categories: []
---

In the good old days the measure of a programmer was efficiency - how much functionality
could be packed into how much space. Languages like *C* keep code close to the machine
and require close attention to, and strong understanding of, machine operation for
performance and code execution.

Much like with the catapult, new methods have come along for launching higher-delivery
projectiles at high speeds, but, when it comes to hurling a VW beetle the length of a
football field, sometimes the old ways are still the best. The importance of efficiency in
code has been maligned, and largely obfuscated, by modern delivery mechanisms; however, its
effect remains critical to the performance of complex large-scale applications.

<!-- more -->
 
### How to crash efficiently

Languages that execute code via a virtualization layer (like Java uses the Java
Virtual Machine (JVM)) bring a host of risks and benefits to the table. One of these soft
features is the additional abstraction from actual machine execution that is inherent in
the architecture. Among other uses, this provides the programmer, or friendly neighborhood
SysOp, a protective buffer from crashing the parent host.

Note: The following code samples illustrate possible problems, so don't *really* run them!

Executing the following C code on a Linux system wold give you a new appreciation of  the
power and responsibility of direct access to buffers, for example:

```
  #include <unistd.h>

  int main(void)
  {
      while(1)
          fork();
  }
```

So a JVM will never crash? Five minutes of creative development will demonstrate otherwise!
The following Java code can result in the marvelously familiar out-of-memory errors and
inevitable interruption:

```
  public class Crash {
      public static void main(String[] args) {
          Object[] o = null;

          while (true) {
              o = new Object[] {o};
          }
      }
  }
```

So a JVM will never crash the OS? It may take closer to 7 minutes, but also not true. See
the following Java Code example:

```
public class BadDay {
   public static void main(String[] args) throws java.io.IOException {
      while(true) {
         Runtime.getRuntime().exec(new String[]{"java", "-cp", System.getProperty("java.class.path"), "BadDay"});
      }
   }
}
```

If both models can bring down a server, then is there a real difference? Yes and no – the
difference is really a product of programmer habits, methodology, and soft education,
rather than of the architecture or capabilities of one language over another.
 
### Habits

Programmers are lazy (sorry, programmers). It’s the guiding tenant of everyone who enters
the technology ecosystem. If finding an easier way wasn’t the goal, then why spend 1m
man-hours building a software mousetrap?

Consider the Uber application. Fundamentally, it operates under the principle that it’s too
complicated, or time consuming, to find a driving service, and that riders and drivers
can be connected via a simple interface. The outcome is that lazy riders, almost certainly
including its programmers, expend less effort in accomplishing A-to-B.

So it’s a good thing? It certainly can be, but the habit doesn’t stop at entrepreneurial
chauffeur services.

Cogitate on the purpose of garbage collection (GC) in programming languages. Well, useless
objects accumulate in memory and have to be cleaned up. That's common sense. No question,
but compare garbage collection in native C to Java, and it becomes rapidly apparent that the
more modern, Java found a lazy way to accomplish A-to-B. Programmers not being required to
contemplate and implement garbage collection has saved countless man-hours. However, another
outcome also entered the picture – programmers started to not care about “how”. So what,
the JVM takes care of GC already, right?
 
### Methodology

Whether paid by lines of code (in a programmer-perfect world), project delivery cost,
delivery timeline, or any other number of metrics, every programmer is developing code
against a specific requirement target. Unless coding the shutter control algorithm for
Discovery’s camera system, operational efficiency and execution footprint are not overly
likely to be at the top of the list of requirements. That’s not inherently wrong.
Businesses have different priorities, and modern computing systems have moved mountains to
reduce the importance of efficiency. The upshot is the *write code that works* approach to
projects and a focus on results. That being said, a program that creates 10% more objects
to accomplish the same operation is likely to result in less efficient GC. 

### Soft Education

One of the most significant shifts in software programming over the past decade or two is
the integrated development environment (IDE). From syntax highlighting to tab completion,
these powerful tools help developers to be better at being lazy as well as reducing time
to delivery. Both are very important in a results-first approach. Another critical
side-feature of the IDE is the increased ease with which large project teams can collaborate.

Collaboration – that’s definitely a good thing, isn’t it? When it comes to programming
security systems for an island-bound dinosaur park, collaboration certainly helps to reduce
the dependency on a single developer. Unfortunately, fluid collaboration also carries a
significant risk – obfuscation.

In object-oriented programming (OOP), the tools of collaboration rise to the surface with
ease - one widget object needs to combine with another widget object to create a
super-widget object. Everyone wants a super-widget, after all, so what’s the problem?
Provided that the two widgets are able to communicate, there is really no need for either
development team to wonder as to how the respective sub-programs actually work, nor at
their efficiency, but a chain is only as strong as its weakest link.
 
### What does any of this have to do with JVM sizing?

The logical size of a JVM is a combination of several metrics, including the following:

* *Minimum Heap*
* *Maximum Heap*
* *Permanent Space (max)*
* *External Operations*
* *Garbage Collection*

One of this author’s most common arguments opportunities for collaborative discussion
involves the phrase “max heap is ___ MB, so why does the system keep running out of memory?”
The truth is that there is no single setting, nor value, that enforces a maximum JVM size,
other than the combined physical memory and swap space of its host OS. So could any JVM  
runaway and take all of the system memory? No, but it’s most common for JVM application
servers to be sized based on the expected JVM size plus a small amount for the OS overhead.

What’s the point of setting the 2 dozen flags for the JVM if none of them control the
memory footprint? The purpose is to set boundaries, similar to having multiple file systems
divided across partitions on a server. When developing the “thunderdome.java” application,
competition to the death would be an exciting feature. For all other applications, it’s
important that the heap not clobber the permanent space in its quest for one more MB. The
most important distinction is between conceptually guaranteed space versus a limit. 

### The Kit and Caboodle

JVM reliability, as a product of not being killed by OOM, is critical to any business
practice. That’s all well and good, but even the most stable applications need to be
restarted at times, so what does this have to do with memory efficiency?

A strong recommendation by the modern JVM community is to run with equivalent settings
for minimum and maximum head (Xms and Xmx). That’s because front-loading the largest memory
resource of the JVM at start means avoiding the slow, and potentially unreliable, growth
from min to max. In addition to that benefit, this type of configuration also means that
restarting the JVM requires full memory allocation before the JVM, and its applications,
become functional. The gap between a 512M and 1GB head seems, and on modern systems
typically is, trivial. Consider, however, the time required to statically assign and address
4GB versus 8GB of RAM. Add in a bit of overhead for paravirtualization in the modern market
place and JVM heap size starts to look really important to maintenance operation planning.

No matter how well written, tested, and supported the application, there is almost always
a risk of unexpected crashes and performance degradation. That’s what a thread dump is for,
right? In many cases, yes. While we’re considering time-to-recovery of an application, let’s
add in the difference between dumping 4GB of addressed memory to file against the same 8GB
above. The beefy 8GB application is starting to look very time-expensive under pressure,
precisely when time costs are most critical.
 
### Compartmentalize the Complexity

If an E-Commerce store could be efficiently run in BASIC, then there would probably be a
market demand. Lacking the latter, it could be that more complex but comprehensive frameworks,
such as Java, are in demand because they solve the needs of the former.

Just because the goal, and comprehensive operation, of an application is very complex,
however, does not necessitate that a single application thread perform all functions. For
example, it would be very straightforward to write code to perform catalog search
optimization. Since the owner of SOLR already did that with their SOLR application, there is
seldom need to reinvent that wheel. The same principle can be applied for other operational
components. Probably true, but why?

Density is a very important modern computing concepts: how many VMs can fit on a hypervisor?
How many products can squeeze into the database? How many concurrent visitors can pack into
the application? In large part, density is about vertical scaling. For many aspects of the
architecture, this is a critical sub-topic. When targeting an efficient JVM, however, there
is a break-even point unique to each application after which vertical scaling on the host
system (like adding memory resources to the heap) have diminishing incremental returns.

It’s at that point, or ideally right before, that the application begins to scale
horizontally instead. From a purely load-based perspective, this is very simple: *One more
user is beyond the threshold, so it’s time to add another server*. This same thought-exercise
can be applied to JVM scaling as well: *One more MB of heap will be inefficient, so it’s time
to add another JVM instance*.

The distinction, however, is that non-load-based horizontal scaling for the JVM means a
division of labor. Imagine an application for ordering widget1 and widget2 from a catalog.
At first, a single JVM can efficiently manage the entire order process. As the
market for widgets expands it becomes necessary to distinguish between visitors and
conversions (sessions that are only browsing the catalog versus sessions that are in
the process of placing an order). Extending the thought-exercise further, when demand becomes
so great that a single JVM cannot manage the catalog for both widget1 and widget2, then an
optimization can be sought for more specific functional compartmentalization focused on each
widget individually.
 
### Bring ‘em home, Casey

As the vehicles to develop and deliver software solutions become more complex, which shows
no sign of slowing, the surface for introducing inefficiencies continues to expand. Doomsday
predictions aside, the most important question to ask during the development and support
lifecycles is “why?”:

* Why does the software require 8GB of RAM instead of 4GB?
* Why does it take 45 minutes to restart the cart application?
* Why is it so hard to find the source of a problem from a thread dump?
 
TL;DR – start as small as possible without compromising performance, then try to perform
when smaller than that.
 
Good Luck!
